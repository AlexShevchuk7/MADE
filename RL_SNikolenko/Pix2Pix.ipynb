{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "worst-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as VF\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms.Lambda(lambda img: (torch.tensor(np.array(img), dtype=torch.float)[:, :256, :], \n",
    "                              torch.tensor(np.array(img), dtype=torch.float)[:, 256:, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "historical-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "\n",
    "dataset = datasets.ImageFolder('train_images', transform=transform)\n",
    "val_dataset = datasets.ImageFolder('val_images', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "broad-washington",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aggressive-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "flexible-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "joint-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "##############################\n",
    "#           U-NET\n",
    "##############################\n",
    "\n",
    "\n",
    "class UNetDown(nn.Module):\n",
    "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n",
    "        super(UNetDown, self).__init__()\n",
    "        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n",
    "        if normalize:\n",
    "            layers.append(nn.InstanceNorm2d(out_size))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class UNetUp(nn.Module):\n",
    "    def __init__(self, in_size, out_size, dropout=0.0):\n",
    "        super(UNetUp, self).__init__()\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(out_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        x = self.model(x)\n",
    "        x = torch.cat((x, skip_input), 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GeneratorUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super(GeneratorUNet, self).__init__()\n",
    "\n",
    "        self.down1 = UNetDown(in_channels, 64, normalize=False)\n",
    "        self.down2 = UNetDown(64, 128)\n",
    "        self.down3 = UNetDown(128, 256)\n",
    "        self.down4 = UNetDown(256, 512, dropout=0.5)\n",
    "        self.down5 = UNetDown(512, 512, dropout=0.5)\n",
    "        self.down6 = UNetDown(512, 512, dropout=0.5)\n",
    "        self.down7 = UNetDown(512, 512, dropout=0.5)\n",
    "        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n",
    "\n",
    "        self.up1 = UNetUp(512, 512, dropout=0.5)\n",
    "        self.up2 = UNetUp(1024, 512, dropout=0.5)\n",
    "        self.up3 = UNetUp(1024, 512, dropout=0.5)\n",
    "        self.up4 = UNetUp(1024, 512, dropout=0.5)\n",
    "        self.up5 = UNetUp(1024, 256)\n",
    "        self.up6 = UNetUp(512, 128)\n",
    "        self.up7 = UNetUp(256, 64)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(128, out_channels, 4, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # U-Net generator with skip connections from encoder to decoder\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "        u1 = self.up1(d8, d7)\n",
    "        u2 = self.up2(u1, d6)\n",
    "        u3 = self.up3(u2, d5)\n",
    "        u4 = self.up4(u3, d4)\n",
    "        u5 = self.up5(u4, d3)\n",
    "        u6 = self.up6(u5, d2)\n",
    "        u7 = self.up7(u6, d1)\n",
    "\n",
    "        return self.final(u7)\n",
    "\n",
    "\n",
    "##############################\n",
    "#        Discriminator\n",
    "##############################\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(in_channels * 2, 64, normalization=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_A, img_B):\n",
    "        # Concatenate image and condition image by channels to produce input\n",
    "        img_input = torch.cat((img_A, img_B), 1)\n",
    "        return self.model(img_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-valentine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "harmful-librarian",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 0/400] [D loss: 1.535743] [G loss: 74.877800, pixel: 0.731446, adv: 1.733238] ETA: 21:56:04.290619\n",
      "[Epoch 0/100] [Batch 1/400] [D loss: 2.128437] [G loss: 71.994637, pixel: 0.698849, adv: 2.109776] ETA: 23:29:59.201419\n",
      "[Epoch 0/100] [Batch 2/400] [D loss: 2.246799] [G loss: 67.437668, pixel: 0.649170, adv: 2.520687] ETA: 1 day, 1:34:13.721898\n",
      "[Epoch 0/100] [Batch 3/400] [D loss: 1.631126] [G loss: 56.127113, pixel: 0.547190, adv: 1.408149] ETA: 20:00:17.255695\n",
      "[Epoch 0/100] [Batch 4/400] [D loss: 1.727078] [G loss: 53.590683, pixel: 0.518232, adv: 1.767525] ETA: 19:44:43.818031\n",
      "[Epoch 0/100] [Batch 5/400] [D loss: 3.136618] [G loss: 57.604401, pixel: 0.526777, adv: 4.926655] ETA: 19:03:57.964592\n",
      "[Epoch 0/100] [Batch 6/400] [D loss: 2.328010] [G loss: 46.968796, pixel: 0.445398, adv: 2.428969] ETA: 18:32:50.283095\n",
      "[Epoch 0/100] [Batch 7/400] [D loss: 1.509299] [G loss: 47.290497, pixel: 0.458047, adv: 1.485766] ETA: 18:34:45.904536\n",
      "[Epoch 0/100] [Batch 8/400] [D loss: 2.641630] [G loss: 44.520054, pixel: 0.418696, adv: 2.650414] ETA: 18:43:19.437414\n",
      "[Epoch 0/100] [Batch 9/400] [D loss: 1.295614] [G loss: 51.972038, pixel: 0.505423, adv: 1.429727] ETA: 18:38:44.404718\n",
      "[Epoch 0/100] [Batch 10/400] [D loss: 1.658084] [G loss: 44.629082, pixel: 0.424883, adv: 2.140745] ETA: 18:36:17.022157\n",
      "[Epoch 0/100] [Batch 11/400] [D loss: 1.145666] [G loss: 42.287903, pixel: 0.409594, adv: 1.328483] ETA: 19:08:01.770991\n",
      "[Epoch 0/100] [Batch 12/400] [D loss: 1.306696] [G loss: 58.585712, pixel: 0.572113, adv: 1.374374] ETA: 19:42:58.055510\n",
      "[Epoch 0/100] [Batch 13/400] [D loss: 1.563874] [G loss: 51.240547, pixel: 0.493832, adv: 1.857363] ETA: 18:42:42.161993\n",
      "[Epoch 0/100] [Batch 14/400] [D loss: 1.118140] [G loss: 41.752045, pixel: 0.404940, adv: 1.258007] ETA: 18:11:56.173402\n",
      "[Epoch 0/100] [Batch 15/400] [D loss: 0.809506] [G loss: 52.046295, pixel: 0.507517, adv: 1.294587] ETA: 18:16:30.910965\n",
      "[Epoch 0/100] [Batch 16/400] [D loss: 0.807194] [G loss: 32.280113, pixel: 0.314475, adv: 0.832618] ETA: 19:22:05.659595\n",
      "[Epoch 0/100] [Batch 17/400] [D loss: 0.729802] [G loss: 39.089931, pixel: 0.382163, adv: 0.873648] ETA: 20:33:07.058671\n",
      "[Epoch 0/100] [Batch 18/400] [D loss: 0.841029] [G loss: 38.871567, pixel: 0.377459, adv: 1.125649] ETA: 19:05:36.058861\n",
      "[Epoch 0/100] [Batch 19/400] [D loss: 0.806478] [G loss: 34.911682, pixel: 0.338201, adv: 1.091575] ETA: 18:07:57.101277\n",
      "[Epoch 0/100] [Batch 20/400] [D loss: 0.626512] [G loss: 31.620104, pixel: 0.304782, adv: 1.141885] ETA: 20:32:33.816895\n",
      "[Epoch 0/100] [Batch 21/400] [D loss: 0.651727] [G loss: 29.637306, pixel: 0.287038, adv: 0.933525] ETA: 18:51:58.968968\n",
      "[Epoch 0/100] [Batch 22/400] [D loss: 0.750871] [G loss: 28.068348, pixel: 0.272098, adv: 0.858510] ETA: 20:19:54.145658\n",
      "[Epoch 0/100] [Batch 23/400] [D loss: 0.854798] [G loss: 34.990738, pixel: 0.335773, adv: 1.413426] ETA: 19:18:59.888486\n",
      "[Epoch 0/100] [Batch 24/400] [D loss: 0.842024] [G loss: 42.176392, pixel: 0.409584, adv: 1.218004] ETA: 19:20:12.224085\n",
      "[Epoch 0/100] [Batch 25/400] [D loss: 0.666124] [G loss: 29.657234, pixel: 0.286345, adv: 1.022743] ETA: 20:03:12.886430\n",
      "[Epoch 0/100] [Batch 26/400] [D loss: 0.729069] [G loss: 34.361961, pixel: 0.333365, adv: 1.025427] ETA: 20:36:24.660671\n",
      "[Epoch 0/100] [Batch 27/400] [D loss: 0.868557] [G loss: 38.420647, pixel: 0.372325, adv: 1.188166] ETA: 19:37:58.902125\n",
      "[Epoch 0/100] [Batch 28/400] [D loss: 0.937434] [G loss: 66.075897, pixel: 0.646715, adv: 1.404350] ETA: 19:39:44.680770\n",
      "[Epoch 0/100] [Batch 29/400] [D loss: 0.687885] [G loss: 49.506172, pixel: 0.482314, adv: 1.274763] ETA: 19:03:56.744768\n",
      "[Epoch 0/100] [Batch 30/400] [D loss: 1.176984] [G loss: 46.297268, pixel: 0.450994, adv: 1.197881] ETA: 19:27:10.426497\n",
      "[Epoch 0/100] [Batch 31/400] [D loss: 0.996774] [G loss: 34.043926, pixel: 0.327407, adv: 1.303219] ETA: 20:20:44.392436\n",
      "[Epoch 0/100] [Batch 32/400] [D loss: 0.591519] [G loss: 32.654190, pixel: 0.317051, adv: 0.949073] ETA: 19:10:33.969635\n",
      "[Epoch 0/100] [Batch 33/400] [D loss: 0.704413] [G loss: 45.403645, pixel: 0.444320, adv: 0.971653] ETA: 20:25:52.016676\n",
      "[Epoch 0/100] [Batch 34/400] [D loss: 0.917989] [G loss: 47.997932, pixel: 0.469431, adv: 1.054831] ETA: 20:28:31.591467\n",
      "[Epoch 0/100] [Batch 35/400] [D loss: 0.748714] [G loss: 32.907978, pixel: 0.315789, adv: 1.329030] ETA: 19:52:24.123167\n",
      "[Epoch 0/100] [Batch 36/400] [D loss: 0.678046] [G loss: 62.050308, pixel: 0.612074, adv: 0.842876] ETA: 20:04:34.772192\n",
      "[Epoch 0/100] [Batch 37/400] [D loss: 0.783590] [G loss: 30.026215, pixel: 0.291909, adv: 0.835319] ETA: 20:10:33.376388\n",
      "[Epoch 0/100] [Batch 38/400] [D loss: 0.525527] [G loss: 37.694004, pixel: 0.367301, adv: 0.963877] ETA: 20:15:04.679441\n",
      "[Epoch 0/100] [Batch 39/400] [D loss: 0.619017] [G loss: 68.788490, pixel: 0.678203, adv: 0.968207] ETA: 20:07:43.592247\n",
      "[Epoch 0/100] [Batch 40/400] [D loss: 0.590068] [G loss: 64.901054, pixel: 0.639615, adv: 0.939524] ETA: 20:20:33.187265\n",
      "[Epoch 0/100] [Batch 41/400] [D loss: 0.543002] [G loss: 61.567619, pixel: 0.602678, adv: 1.299842] ETA: 19:54:52.549204\n",
      "[Epoch 0/100] [Batch 42/400] [D loss: 0.514111] [G loss: 42.418133, pixel: 0.416379, adv: 0.780234] ETA: 20:24:53.364717\n",
      "[Epoch 0/100] [Batch 43/400] [D loss: 0.509178] [G loss: 42.135487, pixel: 0.414477, adv: 0.687750] ETA: 20:22:07.888912\n",
      "[Epoch 0/100] [Batch 44/400] [D loss: 0.396291] [G loss: 49.541325, pixel: 0.485365, adv: 1.004787] ETA: 21:06:04.825753\n",
      "[Epoch 0/100] [Batch 45/400] [D loss: 0.544443] [G loss: 20.885881, pixel: 0.200309, adv: 0.855001] ETA: 20:03:32.427914\n",
      "[Epoch 0/100] [Batch 46/400] [D loss: 0.353519] [G loss: 37.442673, pixel: 0.366123, adv: 0.830376] ETA: 19:55:45.419778\n",
      "[Epoch 0/100] [Batch 47/400] [D loss: 0.425155] [G loss: 42.797836, pixel: 0.416864, adv: 1.111454] ETA: 20:12:49.114867\n",
      "[Epoch 0/100] [Batch 48/400] [D loss: 0.367590] [G loss: 31.186800, pixel: 0.303093, adv: 0.877481] ETA: 20:05:53.238274\n",
      "[Epoch 0/100] [Batch 49/400] [D loss: 0.317181] [G loss: 27.087118, pixel: 0.263538, adv: 0.733342] ETA: 19:29:49.362327\n",
      "[Epoch 0/100] [Batch 50/400] [D loss: 0.261684] [G loss: 67.901733, pixel: 0.670834, adv: 0.818341] ETA: 22:28:31.517072\n",
      "[Epoch 0/100] [Batch 51/400] [D loss: 0.370875] [G loss: 42.962818, pixel: 0.420044, adv: 0.958423] ETA: 19:53:47.413364\n",
      "[Epoch 0/100] [Batch 52/400] [D loss: 0.411808] [G loss: 42.920547, pixel: 0.417354, adv: 1.185197] ETA: 20:17:57.930676\n",
      "[Epoch 0/100] [Batch 53/400] [D loss: 0.383967] [G loss: 30.249577, pixel: 0.289258, adv: 1.323803] ETA: 19:07:46.569879\n",
      "[Epoch 0/100] [Batch 54/400] [D loss: 0.253403] [G loss: 36.640434, pixel: 0.357145, adv: 0.925936] ETA: 19:23:42.794725\n",
      "[Epoch 0/100] [Batch 55/400] [D loss: 0.402709] [G loss: 53.519970, pixel: 0.526512, adv: 0.868762] ETA: 20:28:19.726120\n",
      "[Epoch 0/100] [Batch 56/400] [D loss: 0.251124] [G loss: 28.446714, pixel: 0.274403, adv: 1.006457] ETA: 19:58:38.854376\n",
      "[Epoch 0/100] [Batch 57/400] [D loss: 0.380871] [G loss: 38.970325, pixel: 0.379704, adv: 0.999892] ETA: 21:04:16.877638\n",
      "[Epoch 0/100] [Batch 58/400] [D loss: 0.551719] [G loss: 73.655792, pixel: 0.724131, adv: 1.242733] ETA: 20:09:58.360645\n",
      "[Epoch 0/100] [Batch 59/400] [D loss: 0.681487] [G loss: 41.057030, pixel: 0.394057, adv: 1.651350] ETA: 20:25:47.401341\n",
      "[Epoch 0/100] [Batch 60/400] [D loss: 0.315413] [G loss: 32.618206, pixel: 0.315784, adv: 1.039840] ETA: 19:56:08.825359\n",
      "[Epoch 0/100] [Batch 61/400] [D loss: 0.597519] [G loss: 44.760696, pixel: 0.437432, adv: 1.017515] ETA: 20:52:57.499463\n",
      "[Epoch 0/100] [Batch 62/400] [D loss: 0.506480] [G loss: 47.928837, pixel: 0.468590, adv: 1.069865] ETA: 20:11:34.927254\n",
      "[Epoch 0/100] [Batch 63/400] [D loss: 0.312345] [G loss: 43.896435, pixel: 0.425849, adv: 1.311563] ETA: 20:02:09.820983\n",
      "[Epoch 0/100] [Batch 64/400] [D loss: 0.448254] [G loss: 50.629093, pixel: 0.491960, adv: 1.433100] ETA: 22:13:28.804688\n",
      "[Epoch 0/100] [Batch 65/400] [D loss: 0.349210] [G loss: 29.970186, pixel: 0.288338, adv: 1.136406] ETA: 21:58:06.810871\n",
      "[Epoch 0/100] [Batch 66/400] [D loss: 0.240257] [G loss: 31.026794, pixel: 0.300045, adv: 1.022299] ETA: 20:21:12.836163\n",
      "[Epoch 0/100] [Batch 67/400] [D loss: 0.480192] [G loss: 20.624897, pixel: 0.195545, adv: 1.070442] ETA: 20:24:22.416377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 68/400] [D loss: 0.262877] [G loss: 44.289925, pixel: 0.433861, adv: 0.903860] ETA: 19:57:33.520674\n",
      "[Epoch 0/100] [Batch 69/400] [D loss: 0.253105] [G loss: 48.183781, pixel: 0.471489, adv: 1.034861] ETA: 19:32:59.608001\n",
      "[Epoch 0/100] [Batch 70/400] [D loss: 0.269272] [G loss: 45.029507, pixel: 0.438439, adv: 1.185568] ETA: 20:50:26.801934\n",
      "[Epoch 0/100] [Batch 71/400] [D loss: 0.274529] [G loss: 39.402306, pixel: 0.385060, adv: 0.896282] ETA: 20:48:29.438095\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-baeebd2de2e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mset_requires_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0moptimizer_G\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[0mset_requires_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    116\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m                    eps=group['eps'])\n\u001b[0m\u001b[0;32m    119\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loss functions\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_pixelwise = torch.nn.L1Loss()\n",
    "\n",
    "# Calculate output of image discriminator (PatchGAN)\n",
    "patch = (1, 16, 16)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = GeneratorUNet()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "\n",
    "def set_requires_grad(net, req_grad):\n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = req_grad\n",
    "\n",
    "def sample_images(batches_done):\n",
    "    \"\"\"Saves a generated sample from the validation set\"\"\"\n",
    "    imgs = next(iter(val_dataloader))\n",
    "    real_A = Variable(imgs[\"B\"].type(Tensor))\n",
    "    real_B = Variable(imgs[\"A\"].type(Tensor))\n",
    "    fake_B = generator(real_A)\n",
    "    img_sample = torch.cat((real_A.data, fake_B.data, real_B.data), -2)\n",
    "    save_image(img_sample, \"images/%s/%s.png\" % (opt.dataset_name, batches_done), nrow=5, normalize=True)\n",
    "\n",
    "\n",
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "prev_time = time.time()\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "\n",
    "        # Model inputs\n",
    "        real_A = batch[0][:, :, :, :256]#Variable(batch[\"B\"].type(Tensor))\n",
    "        real_B = batch[0][:, :, :, 256:]#Variable(batch[\"A\"].type(Tensor))\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(torch.tensor(np.ones((real_A.size(0), *patch)), dtype=torch.float), requires_grad=False)\n",
    "        fake = Variable(torch.tensor(np.zeros((real_A.size(0), *patch)), dtype=torch.float), requires_grad=False)\n",
    "\n",
    "        # ------------------\n",
    "        #  Train Generator\n",
    "        # ------------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # GAN loss\n",
    "        fake_B = generator(real_A)\n",
    "        pred_fake = discriminator(fake_B, real_A)\n",
    "        loss_GAN = criterion_GAN(pred_fake, valid)\n",
    "        # Pixel-wise loss\n",
    "        loss_pixel = criterion_pixelwise(fake_B, real_B)\n",
    "\n",
    "        # Total loss\n",
    "        loss_G = loss_GAN + 100 * loss_pixel\n",
    "\n",
    "        loss_G.backward()\n",
    "\n",
    "        set_requires_grad(discriminator, False)\n",
    "        optimizer_G.step()\n",
    "        set_requires_grad(discriminator, True)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real = discriminator(real_B, real_A)\n",
    "        loss_real = criterion_GAN(pred_real, valid)\n",
    "\n",
    "        # Fake loss\n",
    "        pred_fake = discriminator(fake_B.detach(), real_A)\n",
    "        loss_fake = criterion_GAN(pred_fake, fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D = 0.5 * (loss_real + loss_fake)\n",
    "\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        batches_left = n_epochs * len(dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # Print log\n",
    "        print(\n",
    "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, pixel: %f, adv: %f] ETA: %s\"\n",
    "            % (\n",
    "                epoch,\n",
    "                n_epochs,\n",
    "                i,\n",
    "                len(dataloader),\n",
    "                loss_D.item(),\n",
    "                loss_G.item(),\n",
    "                loss_pixel.item(),\n",
    "                loss_GAN.item(),\n",
    "                time_left,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # If at sample interval save image\n",
    "        #if batches_done % opt.sample_interval == 0:\n",
    "        #    sample_images(batches_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = next(iter(val_dataloader))\n",
    "ix = np.random.randint(0, len(val_dataset), 3)\n",
    "real_A = imgs[:, :, :, :256]\n",
    "real_B = imgs[:, :, :, 256:]\n",
    "fake_B = generator(real_A)\n",
    "img_sample = torch.cat((real_A.data, fake_B.data, real_B.data), -2)\n",
    "save_image(img_sample, \"images/%s/%s.png\" % (opt.dataset_name, batches_done), nrow=5, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "peripheral-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = next(iter(val_dataloader))\n",
    "ix = np.random.randint(0, len(val_dataset), 3).tolist()\n",
    "real_A = imgs[0][ix, :, :, :256]\n",
    "real_B = imgs[0][ix, :, :, 256:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "whole-stereo",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_B = generator(real_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "color-addition",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(fake_B, \"images/%s.png\" % ('facades_' +str(1)), nrow=5, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "formal-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = VF.to_pil_image(fake_B.reshape((3, 256, 256 * 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-asian",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.asarray(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-commitment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
