{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "revised-ancient",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet_envs\n",
    "# Don't forget to install PyBullet!\n",
    "from gym import make\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "import random\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = make(ENV_NAME)\n",
    "    ppo = PPO(state_dim=env.observation_space.shape[0], action_dim=env.action_space.shape[0])\n",
    "    state = env.reset()\n",
    "    episodes_sampled = 0\n",
    "    steps_sampled = 0\n",
    "    \n",
    "    for i in range(ITERATIONS):\n",
    "        trajectories = []\n",
    "        steps_ctn = 0\n",
    "        \n",
    "        while len(trajectories) < MIN_EPISODES_PER_UPDATE or steps_ctn < MIN_TRANSITIONS_PER_UPDATE:\n",
    "            traj = sample_episode(env, ppo)\n",
    "            steps_ctn += len(traj)\n",
    "            trajectories.append(traj)\n",
    "        episodes_sampled += len(trajectories)\n",
    "        steps_sampled += steps_ctn\n",
    "\n",
    "        ppo.update(trajectories)        \n",
    "        \n",
    "        if (i + 1) % (ITERATIONS//100) == 0:\n",
    "            rewards = evaluate_policy(env, ppo, 5)\n",
    "            print(f\"Step: {i+1}, Reward mean: {np.mean(rewards)}, Reward std: {np.std(rewards)}, Episodes: {episodes_sampled}, Steps: {steps_sampled}\")\n",
    "            ppo.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "welsh-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"Walker2DBulletEnv-v0\"\n",
    "\n",
    "LAMBDA = 0.95\n",
    "GAMMA = 0.99\n",
    "\n",
    "ACTOR_LR = 2e-4\n",
    "CRITIC_LR = 1e-4\n",
    "\n",
    "CLIP = 0.2\n",
    "ENTROPY_COEF = 1e-2\n",
    "BATCHES_PER_UPDATE = 64\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "MIN_TRANSITIONS_PER_UPDATE = 2048\n",
    "MIN_EPISODES_PER_UPDATE = 4\n",
    "\n",
    "ITERATIONS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fatal-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lambda_returns_and_gae(trajectory):\n",
    "    lambda_returns = []\n",
    "    gae = []\n",
    "    last_lr = 0.\n",
    "    last_v = 0.\n",
    "    for _, _, r, _, v in reversed(trajectory):\n",
    "        ret = r + GAMMA * (last_v * (1 - LAMBDA) + last_lr * LAMBDA)\n",
    "        last_lr = ret\n",
    "        last_v = v\n",
    "        lambda_returns.append(last_lr)\n",
    "        gae.append(last_lr - v)\n",
    "    \n",
    "    # Each transition contains state, action, old action probability, value estimation and advantage estimation\n",
    "    return [(s, a, p, v, adv) for (s, a, _, p, _), v, adv in zip(trajectory, reversed(lambda_returns), reversed(gae))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-circulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    def get_action_distribution(self, state):\n",
    "        mu, log_sigma = torch.chunk(self.model(state), 2, dim=-1)\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        return Normal(mu, sigma) # batch_size x action_size\n",
    "\n",
    "    def get_logprob(self, state, action):\n",
    "        distrib = self.get_action_distribution(state)\n",
    "        return distrib.log_prob(action).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "tracked-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=256):\n",
    "        super().__init__()\n",
    "        # Advice: use same log_sigma for all states to improve stability\n",
    "        # You can do this by defining log_sigma as nn.Parameter(torch.zeros(...))\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_size, 2 * action_dim)\n",
    "        )\n",
    "        #self.sigma = torch.zeros(action_dim)\n",
    "        self.sigma = nn.Parameter(torch.zeros(action_dim))\n",
    "        \n",
    "    def compute_proba(self, state, action):\n",
    "        # Returns probability of action according to current policy and distribution of actions\n",
    "        #with torch.no_grad():\n",
    "            #action = torch.Tensor([action]).to('cpu')\n",
    "            #state = torch.tensor([state], dtype=torch.float32).to('cpu')\n",
    "        mu, log_sigma = torch.chunk(self.model(state), 2, dim=-1)\n",
    "        self.sigma = nn.Parameter(torch.exp(log_sigma))\n",
    "        distrib = Normal(mu, self.sigma)        \n",
    "        action_prob = distrib.log_prob(action).sum(-1)\n",
    "        return action_prob, distrib\n",
    "        \n",
    "    def act(self, state):\n",
    "        # Returns an action (with tanh), not-transformed action (without tanh) and distribution of non-transformed actions\n",
    "        # Remember: agent is not deterministic, sample actions from distribution (e.g. Gaussian)\n",
    "        with torch.no_grad():\n",
    "            #state = torch.tensor([state], dtype=torch.float32).to('cpu')\n",
    "            mu, log_sigma = torch.chunk(self.model(state), 2, dim=-1)\n",
    "        #self.sigma = nn.Parameter(torch.exp(log_sigma))\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        distrib = Normal(mu, sigma)\n",
    "        action = distrib.sample().cpu()#.numpy()[0]\n",
    "        return np.tanh(action), action, distrib\n",
    "        \n",
    "        \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "    def get_value(self, state):\n",
    "        return self.model(state)\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.critic = Critic(state_dim)\n",
    "        self.actor_optim = Adam(self.actor.parameters(), ACTOR_LR)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), CRITIC_LR)\n",
    "\n",
    "    def update(self, trajectories):\n",
    "        transitions = [t for traj in trajectories for t in traj] # Turn a list of trajectories into list of transitions\n",
    "        state, action, old_prob, target_value, advantage = zip(*transitions)\n",
    "        state = np.array(state)\n",
    "        action = np.array(action)\n",
    "        old_prob = np.array(old_prob)\n",
    "        target_value = np.array(target_value)\n",
    "        advantage = np.array(advantage)\n",
    "        advnatage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n",
    "        \n",
    "        \n",
    "        for idx_l in range(BATCHES_PER_UPDATE):\n",
    "            idx = np.random.randint(0, len(transitions), BATCH_SIZE) # Choose random batch\n",
    "        #for batch_ofs in range(0, len(transitions), BATCH_SIZE):\n",
    "            #batch_l = batch_ofs + BATCH_SIZE\n",
    "            s = torch.tensor(state[idx]).float()\n",
    "            a = torch.tensor(action[idx]).float()\n",
    "            op = torch.tensor(old_prob[idx]).float() # Probability of the action in state s.t. old policy\n",
    "            targets = torch.tensor(target_value[idx]).float() # Estimated by lambda-returns \n",
    "            adv = torch.tensor(advantage[idx]).float() # Estimated by generalized advantage estimation \n",
    "                       \n",
    "            # TODO: Update critic here\n",
    "            \n",
    "            self.critic_optim.zero_grad()\n",
    "            values = self.critic.get_value(s)\n",
    "            critic_loss = F.mse_loss(values.squeeze(), targets)\n",
    "            critic_loss.backward()\n",
    "            self.critic_optim.step()\n",
    "            \n",
    "             # TODO: Update actor here\n",
    "            self.actor_optim.zero_grad()\n",
    "            log_prob_pi, action_distrib = self.actor.compute_proba(s, a)\n",
    "            r = torch.exp(log_prob_pi - op)\n",
    "            r_clipped = torch.clamp(r, 1.0 - CLIP, 1.0 + CLIP)\n",
    "            \n",
    "            entropy_loss = -action_distrib.entropy()\n",
    "            actor_loss = -(torch.min(r * adv, r_clipped * adv)).mean() + ENTROPY_COEF * entropy_loss\n",
    "            actor_loss.sum().backward()\n",
    "            self.actor_optim.step()\n",
    "            \n",
    "            \n",
    "    def get_value(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(np.array([state])).float()\n",
    "            value = self.critic.get_value(state)\n",
    "        return value.cpu().item()\n",
    "\n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(np.array([state])).float()\n",
    "            action, pure_action, distr = self.actor.act(state)\n",
    "            prob = torch.exp(distr.log_prob(pure_action).sum(-1))\n",
    "        return action.cpu().numpy()[0], pure_action.cpu().numpy()[0], prob.cpu().item()\n",
    "\n",
    "    def save(self):\n",
    "        torch.save(self.actor, \"agent.pkl\")\n",
    "\n",
    "\n",
    "def evaluate_policy(env, agent, episodes=5):\n",
    "    returns = []\n",
    "    for _ in range(episodes):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        total_reward = 0.\n",
    "        \n",
    "        while not done:\n",
    "            state, reward, done, _ = env.step(agent.act(state)[0])\n",
    "            total_reward += reward\n",
    "        returns.append(total_reward)\n",
    "    return returns\n",
    "   \n",
    "\n",
    "def sample_episode(env, agent):\n",
    "    s = env.reset()\n",
    "    d = False\n",
    "    trajectory = []\n",
    "    while not d:\n",
    "        a, pa, p = agent.act(s)\n",
    "        v = agent.get_value(s)\n",
    "        ns, r, d, _ = env.step(a)\n",
    "        trajectory.append((s, pa, r, p, v))\n",
    "        s = ns\n",
    "    return compute_lambda_returns_and_gae(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "italic-variance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0d103601b4498eb18ffda48792dbac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10, Reward mean: 32.13324137181538, Reward std: 14.824891031817376, Episodes: 668, Steps: 20679\n",
      "Step: 20, Reward mean: 53.08222199150507, Reward std: 11.870823899245186, Episodes: 926, Steps: 41739\n",
      "Step: 30, Reward mean: 57.99270657505841, Reward std: 18.64929467187126, Episodes: 1214, Steps: 62632\n",
      "Step: 40, Reward mean: 46.96324923671579, Reward std: 6.491880535246846, Episodes: 1531, Steps: 83393\n",
      "Step: 50, Reward mean: 62.683664823365405, Reward std: 5.618815119914279, Episodes: 1847, Steps: 104195\n",
      "Step: 60, Reward mean: 46.937245299557404, Reward std: 16.61256230690758, Episodes: 2160, Steps: 124999\n",
      "Step: 70, Reward mean: 45.92411787030753, Reward std: 24.014944284207797, Episodes: 2447, Steps: 145871\n",
      "Step: 80, Reward mean: 56.72632210869256, Reward std: 20.591791806520696, Episodes: 2771, Steps: 166696\n",
      "Step: 90, Reward mean: 53.38119336348261, Reward std: 21.370727712765632, Episodes: 3062, Steps: 187388\n",
      "Step: 100, Reward mean: 59.67915498116315, Reward std: 26.603054818780247, Episodes: 3254, Steps: 210213\n",
      "Step: 110, Reward mean: 79.51065648779011, Reward std: 21.845851009938823, Episodes: 3489, Steps: 231033\n",
      "Step: 120, Reward mean: 184.1700230131816, Reward std: 151.49523199180294, Episodes: 3708, Steps: 252308\n",
      "Step: 130, Reward mean: 118.43046405339726, Reward std: 108.05204950949906, Episodes: 3852, Steps: 274261\n",
      "Step: 140, Reward mean: 178.75345124505887, Reward std: 200.02026796933922, Episodes: 3946, Steps: 297678\n",
      "Step: 150, Reward mean: 135.63216840534326, Reward std: 72.32062503301704, Episodes: 4122, Steps: 319445\n",
      "Step: 160, Reward mean: 204.6786862152318, Reward std: 196.77681549654665, Episodes: 4223, Steps: 343774\n",
      "Step: 170, Reward mean: 248.33441131258832, Reward std: 203.0347619324326, Episodes: 4300, Steps: 366549\n",
      "Step: 180, Reward mean: 230.72796376002094, Reward std: 225.32368948116104, Episodes: 4404, Steps: 389128\n",
      "Step: 190, Reward mean: 91.81880016233364, Reward std: 53.742407171372, Episodes: 4521, Steps: 412786\n",
      "Step: 200, Reward mean: 216.33718692336416, Reward std: 207.01850468124272, Episodes: 4666, Steps: 435879\n",
      "Step: 210, Reward mean: 61.09361867653352, Reward std: 13.896774874063706, Episodes: 4817, Steps: 460701\n",
      "Step: 220, Reward mean: 184.5871086983187, Reward std: 142.01263280925147, Episodes: 5059, Steps: 482213\n",
      "Step: 230, Reward mean: 136.36889001260772, Reward std: 140.26687687672077, Episodes: 5242, Steps: 504752\n",
      "Step: 240, Reward mean: 128.21957833973633, Reward std: 86.33995259525203, Episodes: 5373, Steps: 526591\n",
      "Step: 250, Reward mean: 37.20264026218648, Reward std: 13.001146436557931, Episodes: 5606, Steps: 547635\n",
      "Step: 260, Reward mean: 79.88570854953987, Reward std: 27.343288957671064, Episodes: 5846, Steps: 568901\n",
      "Step: 270, Reward mean: 78.28271172388341, Reward std: 46.436540007032434, Episodes: 6067, Steps: 590244\n",
      "Step: 280, Reward mean: 125.50820853620738, Reward std: 62.315042538582354, Episodes: 6223, Steps: 611421\n",
      "Step: 290, Reward mean: 167.1060884137463, Reward std: 120.4131797885599, Episodes: 6335, Steps: 633460\n",
      "Step: 300, Reward mean: 142.89037159783211, Reward std: 42.64673202756668, Episodes: 6502, Steps: 656933\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-166-687d8782a5f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mITERATIONS\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mppo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Step: {i+1}, Reward mean: {np.mean(rewards)}, Reward std: {np.std(rewards)}, Episodes: {episodes_sampled}, Steps: {steps_sampled}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mppo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-165-02de28be1a49>\u001b[0m in \u001b[0;36mevaluate_policy\u001b[1;34m(env, agent, episodes)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[0mreturns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_reward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         ), \"Cannot call env.step() before calling reset()\"\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\lib\\site-packages\\pybullet_envs\\gym_locomotion_envs.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m     67\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscene\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrobot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalc_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# also calculates self.joints_at_limit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     self._alive = float(\n",
      "\u001b[1;32mC:\\Python\\lib\\site-packages\\pybullet_envs\\robot_locomotors.py\u001b[0m in \u001b[0;36mcalc_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                              np.cos(-yaw), 0], [0, 0, 1]])\n\u001b[0;32m     61\u001b[0m     vx, vy, vz = np.dot(rot_speed,\n\u001b[1;32m---> 62\u001b[1;33m                         self.robot_body.speed())  # rotate speed back to body point of view\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     more = np.array(\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = make(ENV_NAME)\n",
    "ppo = PPO(state_dim=env.observation_space.shape[0], action_dim=env.action_space.shape[0])\n",
    "state = env.reset()\n",
    "episodes_sampled = 0\n",
    "steps_sampled = 0\n",
    "\n",
    "for i in tqdm(range(ITERATIONS)):\n",
    "    trajectories = []\n",
    "    steps_ctn = 0\n",
    "\n",
    "    while len(trajectories) < MIN_EPISODES_PER_UPDATE or steps_ctn < MIN_TRANSITIONS_PER_UPDATE:\n",
    "        traj = sample_episode(env, ppo)\n",
    "        steps_ctn += len(traj)\n",
    "        trajectories.append(traj)\n",
    "    episodes_sampled += len(trajectories)\n",
    "    steps_sampled += steps_ctn\n",
    "\n",
    "    ppo.update(trajectories)        \n",
    "\n",
    "    if (i + 1) % (ITERATIONS//100) == 0:\n",
    "        rewards = evaluate_policy(env, ppo, 5)\n",
    "        print(f\"Step: {i+1}, Reward mean: {np.mean(rewards)}, Reward std: {np.std(rewards)}, Episodes: {episodes_sampled}, Steps: {steps_sampled}\")\n",
    "        ppo.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-problem",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-latter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-japan",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
