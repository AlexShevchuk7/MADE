{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "PPO_training.ipynb",
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9d6a218942ac4b609d0335674cdd59f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0f9e06e4d45b4f1b9c2127bd111ffeed",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b7beff7b7564417d96e83ff0548635de",
              "IPY_MODEL_63ae0b54b9ca4a2c88fc342e5760cb48",
              "IPY_MODEL_26dca88e71304af3a98151b1bf060752"
            ]
          }
        },
        "0f9e06e4d45b4f1b9c2127bd111ffeed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b7beff7b7564417d96e83ff0548635de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9ab7c9d6cb3b4906b671fe75a9eabe9b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f7677d2e182440ac81ef495dbfc3a0da"
          }
        },
        "63ae0b54b9ca4a2c88fc342e5760cb48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0bf4b95e0e36424e87abbb39632ea68f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_09631ed0685c4fc988e72483ed307842"
          }
        },
        "26dca88e71304af3a98151b1bf060752": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d4a3e54d1c0d42ea94a56e9b484664cd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1000/1000 [1:26:09&lt;00:00,  7.14s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1b63bc08c2bb437f88f5780c7041d3b8"
          }
        },
        "9ab7c9d6cb3b4906b671fe75a9eabe9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f7677d2e182440ac81ef495dbfc3a0da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0bf4b95e0e36424e87abbb39632ea68f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "09631ed0685c4fc988e72483ed307842": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d4a3e54d1c0d42ea94a56e9b484664cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1b63bc08c2bb437f88f5780c7041d3b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ksnZQl7l7UC",
        "outputId": "c9f1c5d1-5bc0-4352-a29c-7a1fa6124166",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!apt update > /dev/null 2>&1\n",
        "!pip install Box2D  > /dev/null 2>&1\n",
        "!pip install gym[all] > /dev/null 2>&1\n",
        "!pip install tqdm > /dev/null 2>&1\n",
        "!pip install PyBullet"
      ],
      "id": "_ksnZQl7l7UC",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyBullet\n",
            "  Downloading pybullet-3.2.0-cp37-cp37m-manylinux1_x86_64.whl (89.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 89.3 MB 26 kB/s \n",
            "\u001b[?25hInstalling collected packages: PyBullet\n",
            "Successfully installed PyBullet-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "revised-ancient"
      },
      "source": [
        "import pybullet_envs\n",
        "# Don't forget to install PyBullet!\n",
        "from gym import make\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.distributions import Normal\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import Adam\n",
        "import random\n",
        "from tqdm.notebook import tqdm"
      ],
      "id": "revised-ancient",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "welsh-object"
      },
      "source": [
        "ENV_NAME = \"Walker2DBulletEnv-v0\"\n",
        "\n",
        "LAMBDA = 0.95\n",
        "GAMMA = 0.99\n",
        "\n",
        "ACTOR_LR = 1e-4\n",
        "CRITIC_LR = 1e-3\n",
        "\n",
        "CLIP = 0.1\n",
        "ENTROPY_COEF = 1e-2 / 2\n",
        "BATCHES_PER_UPDATE = 64\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "MIN_TRANSITIONS_PER_UPDATE = 2048\n",
        "MIN_EPISODES_PER_UPDATE = 4\n",
        "\n",
        "ITERATIONS = 1000"
      ],
      "id": "welsh-object",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fatal-edmonton"
      },
      "source": [
        "def compute_lambda_returns_and_gae(trajectory):\n",
        "    lambda_returns = []\n",
        "    gae = []\n",
        "    last_lr = 0.\n",
        "    last_v = 0.\n",
        "    for _, _, r, _, v in reversed(trajectory):\n",
        "        ret = r + GAMMA * (last_v * (1 - LAMBDA) + last_lr * LAMBDA)\n",
        "        last_lr = ret\n",
        "        last_v = v\n",
        "        lambda_returns.append(last_lr)\n",
        "        gae.append(last_lr - v)\n",
        "    \n",
        "    # Each transition contains state, action, old action probability, value estimation and advantage estimation\n",
        "    return [(s, a, p, v, adv) for (s, a, _, p, _), v, adv in zip(trajectory, reversed(lambda_returns), reversed(gae))]"
      ],
      "id": "fatal-edmonton",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tracked-scout"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
        "        super().__init__()\n",
        "        # Advice: use same log_sigma for all states to improve stability\n",
        "        # You can do this by defining log_sigma as nn.Parameter(torch.zeros(...))\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_size),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(hidden_size, 2 * action_dim)\n",
        "        )\n",
        "        #self.sigma = torch.zeros(action_dim)\n",
        "        self.sigma = nn.Parameter(torch.zeros(action_dim))\n",
        "        \n",
        "    def compute_proba(self, state, action):\n",
        "        # Returns probability of action according to current policy and distribution of actions\n",
        "        #with torch.no_grad():\n",
        "            #action = torch.Tensor([action]).to('cpu')\n",
        "            #state = torch.tensor([state], dtype=torch.float32).to('cpu')\n",
        "        mu, log_sigma = torch.chunk(self.model(state), 2, dim=-1)\n",
        "        self.sigma = nn.Parameter(torch.exp(log_sigma))\n",
        "        distrib = Normal(mu, self.sigma)        \n",
        "        action_prob = distrib.log_prob(action).sum(-1)\n",
        "        return action_prob, distrib\n",
        "        \n",
        "    def act(self, state):\n",
        "        # Returns an action (with tanh), not-transformed action (without tanh) and distribution of non-transformed actions\n",
        "        # Remember: agent is not deterministic, sample actions from distribution (e.g. Gaussian)\n",
        "        with torch.no_grad():\n",
        "            #state = torch.tensor([state], dtype=torch.float32).to('cpu')\n",
        "            mu, log_sigma = torch.chunk(self.model(state), 2, dim=-1)\n",
        "        #self.sigma = nn.Parameter(torch.exp(log_sigma))\n",
        "        sigma = torch.exp(log_sigma)\n",
        "        distrib = Normal(mu, sigma)\n",
        "        action = distrib.sample().cpu()#.numpy()[0]\n",
        "        return np.tanh(action), action, distrib\n",
        "        \n",
        "        \n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "        \n",
        "    def get_value(self, state):\n",
        "        return self.model(state)\n",
        "\n",
        "\n",
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        self.actor = Actor(state_dim, action_dim)\n",
        "        self.critic = Critic(state_dim)\n",
        "        self.actor_optim = Adam(self.actor.parameters(), ACTOR_LR)\n",
        "        self.critic_optim = Adam(self.critic.parameters(), CRITIC_LR)\n",
        "\n",
        "    def update(self, trajectories):\n",
        "        transitions = [t for traj in trajectories for t in traj] # Turn a list of trajectories into list of transitions\n",
        "        state, action, old_prob, target_value, advantage = zip(*transitions)\n",
        "        state = np.array(state)\n",
        "        action = np.array(action)\n",
        "        old_prob = np.array(old_prob)\n",
        "        target_value = np.array(target_value)\n",
        "        advantage = np.array(advantage)\n",
        "        advnatage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n",
        "        \n",
        "        \n",
        "        for idx_l in range(BATCHES_PER_UPDATE):\n",
        "            idx = np.random.randint(0, len(transitions), BATCH_SIZE) # Choose random batch\n",
        "            s = torch.tensor(state[idx]).float()\n",
        "            a = torch.tensor(action[idx]).float()\n",
        "            op = torch.tensor(old_prob[idx]).float() # Probability of the action in state s.t. old policy\n",
        "            targets = torch.tensor(target_value[idx]).float() # Estimated by lambda-returns \n",
        "            adv = torch.tensor(advantage[idx]).float() # Estimated by generalized advantage estimation \n",
        "                       \n",
        "            # TODO: Update critic here\n",
        "            \n",
        "            self.critic_optim.zero_grad()\n",
        "            values = self.critic.get_value(s)\n",
        "            critic_loss = F.mse_loss(values.squeeze(), targets)\n",
        "            critic_loss.backward()\n",
        "            self.critic_optim.step()\n",
        "            \n",
        "             # TODO: Update actor here\n",
        "            self.actor_optim.zero_grad()\n",
        "            log_prob_pi, action_distrib = self.actor.compute_proba(s, a)\n",
        "            r = torch.exp(log_prob_pi - op.detach())\n",
        "            r_clipped = torch.clamp(r, 1.0 - CLIP, 1.0 + CLIP)\n",
        "            \n",
        "            #entropy_loss = -action_distrib.entropy()\n",
        "            actor_loss = -(torch.min(r * adv, r_clipped * adv)).mean() #+ ENTROPY_COEF * entropy_loss\n",
        "            #actor_loss.sum().backward()\n",
        "            actor_loss.backward()\n",
        "            self.actor_optim.step()\n",
        "            \n",
        "            \n",
        "    def get_value(self, state):\n",
        "        with torch.no_grad():\n",
        "            state = torch.tensor(np.array([state])).float()\n",
        "            value = self.critic.get_value(state)\n",
        "        return value.cpu().item()\n",
        "\n",
        "    def act(self, state):\n",
        "        with torch.no_grad():\n",
        "            state = torch.tensor(np.array([state])).float()\n",
        "            action, pure_action, distr = self.actor.act(state)\n",
        "            prob = torch.exp(distr.log_prob(pure_action).sum(-1))\n",
        "        return action.cpu().numpy()[0], pure_action.cpu().numpy()[0], prob.cpu().item()\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.actor, \"agent.pkl\")\n",
        "\n",
        "\n",
        "def evaluate_policy(env, agent, episodes=5):\n",
        "    returns = []\n",
        "    for _ in range(episodes):\n",
        "        done = False\n",
        "        state = env.reset()\n",
        "        total_reward = 0.\n",
        "        \n",
        "        while not done:\n",
        "            state, reward, done, _ = env.step(agent.act(state)[0])\n",
        "            total_reward += reward\n",
        "        returns.append(total_reward)\n",
        "    return returns\n",
        "   \n",
        "\n",
        "def sample_episode(env, agent):\n",
        "    s = env.reset()\n",
        "    d = False\n",
        "    trajectory = []\n",
        "    while not d:\n",
        "        a, pa, p = agent.act(s)\n",
        "        v = agent.get_value(s)\n",
        "        ns, r, d, _ = env.step(a)\n",
        "        trajectory.append((s, pa, r, p, v))\n",
        "        s = ns\n",
        "    return compute_lambda_returns_and_gae(trajectory)"
      ],
      "id": "tracked-scout",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "italic-variance",
        "outputId": "0629095d-dbf5-4344-fbd4-5403513fdd03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9d6a218942ac4b609d0335674cdd59f5",
            "0f9e06e4d45b4f1b9c2127bd111ffeed",
            "b7beff7b7564417d96e83ff0548635de",
            "63ae0b54b9ca4a2c88fc342e5760cb48",
            "26dca88e71304af3a98151b1bf060752",
            "9ab7c9d6cb3b4906b671fe75a9eabe9b",
            "f7677d2e182440ac81ef495dbfc3a0da",
            "0bf4b95e0e36424e87abbb39632ea68f",
            "09631ed0685c4fc988e72483ed307842",
            "d4a3e54d1c0d42ea94a56e9b484664cd",
            "1b63bc08c2bb437f88f5780c7041d3b8"
          ]
        }
      },
      "source": [
        "env = make(ENV_NAME)\n",
        "ppo = PPO(state_dim=env.observation_space.shape[0], action_dim=env.action_space.shape[0])\n",
        "state = env.reset()\n",
        "episodes_sampled = 0\n",
        "steps_sampled = 0\n",
        "\n",
        "for i in tqdm(range(ITERATIONS)):\n",
        "    trajectories = []\n",
        "    steps_ctn = 0\n",
        "\n",
        "    while len(trajectories) < MIN_EPISODES_PER_UPDATE or steps_ctn < MIN_TRANSITIONS_PER_UPDATE:\n",
        "        traj = sample_episode(env, ppo)\n",
        "        steps_ctn += len(traj)\n",
        "        trajectories.append(traj)\n",
        "    episodes_sampled += len(trajectories)\n",
        "    steps_sampled += steps_ctn\n",
        "\n",
        "    ppo.update(trajectories)        \n",
        "\n",
        "    if (i + 1) % (ITERATIONS//100) == 0:\n",
        "        rewards = evaluate_policy(env, ppo, 5)\n",
        "        print(f\"Step: {i+1}, Reward mean: {np.mean(rewards)}, Reward std: {np.std(rewards)}, Episodes: {episodes_sampled}, Steps: {steps_sampled}\")\n",
        "        ppo.save()"
      ],
      "id": "italic-variance",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d6a218942ac4b609d0335674cdd59f5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 10, Reward mean: 31.24282581332896, Reward std: 11.699720247478897, Episodes: 676, Steps: 20675\n",
            "Step: 20, Reward mean: 55.28070148121361, Reward std: 22.062990981279608, Episodes: 944, Steps: 41451\n",
            "Step: 30, Reward mean: 49.67707429049625, Reward std: 31.403816954917886, Episodes: 1176, Steps: 62713\n",
            "Step: 40, Reward mean: 58.94468520669528, Reward std: 14.54385070515756, Episodes: 1439, Steps: 83773\n",
            "Step: 50, Reward mean: 56.539080473427305, Reward std: 17.548833641612493, Episodes: 1712, Steps: 104698\n",
            "Step: 60, Reward mean: 64.8583438081242, Reward std: 9.44058332563372, Episodes: 1988, Steps: 125578\n",
            "Step: 70, Reward mean: 254.68336873211712, Reward std: 181.73219746363014, Episodes: 2215, Steps: 146612\n",
            "Step: 80, Reward mean: 101.99289765323198, Reward std: 116.880526312341, Episodes: 2363, Steps: 169362\n",
            "Step: 90, Reward mean: 38.7140684670335, Reward std: 18.96466074004317, Episodes: 2460, Steps: 193390\n",
            "Step: 100, Reward mean: 61.1469119948268, Reward std: 30.675777284098793, Episodes: 2693, Steps: 215495\n",
            "Step: 110, Reward mean: 281.41176787561415, Reward std: 247.21425769516992, Episodes: 2823, Steps: 238112\n",
            "Step: 120, Reward mean: 379.7181431941276, Reward std: 233.356479222863, Episodes: 2880, Steps: 263402\n",
            "Step: 130, Reward mean: 186.50673976851587, Reward std: 191.58758170460987, Episodes: 2926, Steps: 290167\n",
            "Step: 140, Reward mean: 381.1954512473072, Reward std: 223.0934724178902, Episodes: 2987, Steps: 314551\n",
            "Step: 150, Reward mean: 371.51289658621954, Reward std: 227.2652961607658, Episodes: 3045, Steps: 340257\n",
            "Step: 160, Reward mean: 367.18895154579934, Reward std: 232.36041917256273, Episodes: 3087, Steps: 371464\n",
            "Step: 170, Reward mean: 345.87869452977907, Reward std: 244.42589728903957, Episodes: 3132, Steps: 399288\n",
            "Step: 180, Reward mean: 240.0562759203245, Reward std: 243.294782040796, Episodes: 3181, Steps: 424733\n",
            "Step: 190, Reward mean: 176.80711830323395, Reward std: 200.2438687397359, Episodes: 3228, Steps: 451251\n",
            "Step: 200, Reward mean: 239.4190878724687, Reward std: 157.5297547740771, Episodes: 3271, Steps: 481870\n",
            "Step: 210, Reward mean: 239.0230198313797, Reward std: 244.44252749783746, Episodes: 3320, Steps: 507340\n",
            "Step: 220, Reward mean: 218.15815800906762, Reward std: 196.7492050506822, Episodes: 3408, Steps: 532186\n",
            "Step: 230, Reward mean: 353.2681926773115, Reward std: 233.92768513198544, Episodes: 3475, Steps: 557027\n",
            "Step: 240, Reward mean: 271.6173794610538, Reward std: 265.5279169437194, Episodes: 3530, Steps: 583394\n",
            "Step: 250, Reward mean: 166.90672056036402, Reward std: 221.65614278817017, Episodes: 3580, Steps: 608218\n",
            "Step: 260, Reward mean: 297.5091130327577, Reward std: 251.86602256358822, Episodes: 3627, Steps: 634888\n",
            "Step: 270, Reward mean: 254.45036256305343, Reward std: 254.77205895468705, Episodes: 3670, Steps: 662875\n",
            "Step: 280, Reward mean: 575.9930694554762, Reward std: 17.376056566650796, Episodes: 3710, Steps: 694315\n",
            "Step: 290, Reward mean: 600.3887831070517, Reward std: 10.54752076981051, Episodes: 3750, Steps: 723980\n",
            "Step: 300, Reward mean: 597.9464440182346, Reward std: 8.963892790606444, Episodes: 3800, Steps: 752864\n",
            "Step: 310, Reward mean: 306.89934603396773, Reward std: 251.72683158595916, Episodes: 3841, Steps: 782346\n",
            "Step: 320, Reward mean: 280.459451307236, Reward std: 254.52851740121037, Episodes: 3881, Steps: 811361\n",
            "Step: 330, Reward mean: 438.51325643788596, Reward std: 198.27309376057747, Episodes: 3923, Steps: 842558\n",
            "Step: 340, Reward mean: 130.82275680707772, Reward std: 91.14842696152675, Episodes: 3966, Steps: 869655\n",
            "Step: 350, Reward mean: 268.30828264476486, Reward std: 269.83082328437985, Episodes: 4011, Steps: 894188\n",
            "Step: 360, Reward mean: 320.4209251827275, Reward std: 188.26860573440723, Episodes: 4053, Steps: 917608\n",
            "Step: 370, Reward mean: 391.822082945243, Reward std: 203.26241781571645, Episodes: 4121, Steps: 941729\n",
            "Step: 380, Reward mean: 175.668468406077, Reward std: 125.72664678732991, Episodes: 4176, Steps: 965706\n",
            "Step: 390, Reward mean: 280.0261306356252, Reward std: 269.3967849627275, Episodes: 4228, Steps: 990088\n",
            "Step: 400, Reward mean: 114.7872832117644, Reward std: 147.8743063775232, Episodes: 4288, Steps: 1015818\n",
            "Step: 410, Reward mean: 290.3548574999553, Reward std: 215.92698483086264, Episodes: 4389, Steps: 1037837\n",
            "Step: 420, Reward mean: 159.6815027764756, Reward std: 119.20137674891086, Episodes: 4513, Steps: 1060340\n",
            "Step: 430, Reward mean: 157.81715998320192, Reward std: 196.39025615547652, Episodes: 4591, Steps: 1082453\n",
            "Step: 440, Reward mean: 190.26357401211783, Reward std: 231.0811024153322, Episodes: 4652, Steps: 1106166\n",
            "Step: 450, Reward mean: 378.58675697287725, Reward std: 270.9137908740967, Episodes: 4709, Steps: 1130056\n",
            "Step: 460, Reward mean: 425.3457370829128, Reward std: 278.3085851571658, Episodes: 4752, Steps: 1156298\n",
            "Step: 470, Reward mean: 309.0213383308926, Reward std: 305.7263101788056, Episodes: 4828, Steps: 1179329\n",
            "Step: 480, Reward mean: 324.7528266206843, Reward std: 292.1832886274921, Episodes: 4884, Steps: 1207756\n",
            "Step: 490, Reward mean: 168.35638855804916, Reward std: 260.4861254956078, Episodes: 4927, Steps: 1234172\n",
            "Step: 500, Reward mean: 199.03748369238951, Reward std: 251.57766979047454, Episodes: 4975, Steps: 1259167\n",
            "Step: 510, Reward mean: 432.0737648718731, Reward std: 289.9236999512447, Episodes: 5019, Steps: 1284214\n",
            "Step: 520, Reward mean: 412.00107182988575, Reward std: 300.04573292538225, Episodes: 5065, Steps: 1307840\n",
            "Step: 530, Reward mean: 451.16752159047957, Reward std: 277.1826478682354, Episodes: 5122, Steps: 1331401\n",
            "Step: 540, Reward mean: 316.24488330041163, Reward std: 294.4345798806708, Episodes: 5169, Steps: 1356966\n",
            "Step: 550, Reward mean: 458.6495863922555, Reward std: 271.9002421998627, Episodes: 5212, Steps: 1383792\n",
            "Step: 560, Reward mean: 301.6242489906614, Reward std: 277.3024178292997, Episodes: 5252, Steps: 1411947\n",
            "Step: 570, Reward mean: 442.7978677566608, Reward std: 253.81615113291528, Episodes: 5300, Steps: 1439710\n",
            "Step: 580, Reward mean: 553.9559535992388, Reward std: 241.19811416828645, Episodes: 5354, Steps: 1466904\n",
            "Step: 590, Reward mean: 429.78763546027204, Reward std: 279.3355763755618, Episodes: 5411, Steps: 1490493\n",
            "Step: 600, Reward mean: 384.99464821732016, Reward std: 260.2875206626887, Episodes: 5463, Steps: 1513628\n",
            "Step: 610, Reward mean: 420.26572587867787, Reward std: 286.4834136607314, Episodes: 5520, Steps: 1539302\n",
            "Step: 620, Reward mean: 388.8010415488009, Reward std: 233.0032571609346, Episodes: 5565, Steps: 1570569\n",
            "Step: 630, Reward mean: 314.04768686596753, Reward std: 294.9793258520913, Episodes: 5614, Steps: 1597077\n",
            "Step: 640, Reward mean: 429.9538900555529, Reward std: 306.2392560541165, Episodes: 5657, Steps: 1623147\n",
            "Step: 650, Reward mean: 471.8738489825716, Reward std: 280.9144232327581, Episodes: 5707, Steps: 1652812\n",
            "Step: 660, Reward mean: 564.3142745099652, Reward std: 235.06519767321652, Episodes: 5754, Steps: 1680274\n",
            "Step: 670, Reward mean: 681.0348065808791, Reward std: 12.627230791826843, Episodes: 5801, Steps: 1708258\n",
            "Step: 680, Reward mean: 392.5101063964457, Reward std: 274.1457762359505, Episodes: 5856, Steps: 1735211\n",
            "Step: 690, Reward mean: 369.1914859623296, Reward std: 270.0010404688952, Episodes: 5899, Steps: 1759653\n",
            "Step: 700, Reward mean: 323.11811259819086, Reward std: 305.85139362163284, Episodes: 5944, Steps: 1790087\n",
            "Step: 710, Reward mean: 381.9810696912032, Reward std: 270.5315237472196, Episodes: 5999, Steps: 1815352\n",
            "Step: 720, Reward mean: 451.9225690634297, Reward std: 294.24114398876776, Episodes: 6041, Steps: 1843078\n",
            "Step: 730, Reward mean: 333.0181271562244, Reward std: 298.5757912247159, Episodes: 6090, Steps: 1869481\n",
            "Step: 740, Reward mean: 262.717796918351, Reward std: 234.6656556283355, Episodes: 6137, Steps: 1895958\n",
            "Step: 750, Reward mean: 464.8005532416253, Reward std: 279.89203146732535, Episodes: 6178, Steps: 1920284\n",
            "Step: 760, Reward mean: 343.50992459678264, Reward std: 274.76037573644095, Episodes: 6231, Steps: 1946212\n",
            "Step: 770, Reward mean: 134.95681569466052, Reward std: 125.47213693348571, Episodes: 6302, Steps: 1971035\n",
            "Step: 780, Reward mean: 446.1350590132527, Reward std: 289.1677559712255, Episodes: 6349, Steps: 1998189\n",
            "Step: 790, Reward mean: 535.8274805108802, Reward std: 233.1363087774862, Episodes: 6405, Steps: 2025365\n",
            "Step: 800, Reward mean: 661.4786855604518, Reward std: 48.969574723418276, Episodes: 6451, Steps: 2051642\n",
            "Step: 810, Reward mean: 213.8816252589391, Reward std: 156.76848492195023, Episodes: 6506, Steps: 2074775\n",
            "Step: 820, Reward mean: 509.5092832830648, Reward std: 199.0347024427914, Episodes: 6553, Steps: 2102229\n",
            "Step: 830, Reward mean: 350.5529388339145, Reward std: 294.96460708397655, Episodes: 6594, Steps: 2132594\n",
            "Step: 840, Reward mean: 470.8592457979148, Reward std: 262.6360391432013, Episodes: 6636, Steps: 2158287\n",
            "Step: 850, Reward mean: 290.48640946730336, Reward std: 234.23707371814515, Episodes: 6701, Steps: 2185268\n",
            "Step: 860, Reward mean: 276.1451557164536, Reward std: 204.229650739064, Episodes: 6758, Steps: 2212520\n",
            "Step: 870, Reward mean: 271.0570211284234, Reward std: 243.0674365391082, Episodes: 6813, Steps: 2237737\n",
            "Step: 880, Reward mean: 579.6462603160535, Reward std: 199.09889334270932, Episodes: 6860, Steps: 2262576\n",
            "Step: 890, Reward mean: 452.42149471458396, Reward std: 283.7632120779849, Episodes: 6901, Steps: 2288929\n",
            "Step: 900, Reward mean: 319.92221655298636, Reward std: 289.7535069005379, Episodes: 6945, Steps: 2318677\n",
            "Step: 910, Reward mean: 329.5511634645574, Reward std: 281.4884437354458, Episodes: 6988, Steps: 2349157\n",
            "Step: 920, Reward mean: 364.13894176444944, Reward std: 218.30778195560714, Episodes: 7037, Steps: 2374852\n",
            "Step: 930, Reward mean: 302.13622810566795, Reward std: 310.70066447715607, Episodes: 7088, Steps: 2400678\n",
            "Step: 940, Reward mean: 560.8364499784626, Reward std: 237.16894092227255, Episodes: 7134, Steps: 2427425\n",
            "Step: 950, Reward mean: 434.022060846572, Reward std: 295.4912282308476, Episodes: 7174, Steps: 2453119\n",
            "Step: 960, Reward mean: 92.40299013999146, Reward std: 33.5910303499407, Episodes: 7226, Steps: 2479500\n",
            "Step: 970, Reward mean: 348.91352665228743, Reward std: 228.88748852819035, Episodes: 7284, Steps: 2505910\n",
            "Step: 980, Reward mean: 87.74028932455607, Reward std: 5.664526253647095, Episodes: 7339, Steps: 2529839\n",
            "Step: 990, Reward mean: 311.88144751287916, Reward std: 259.9640003470149, Episodes: 7408, Steps: 2553523\n",
            "Step: 1000, Reward mean: 552.3014142247102, Reward std: 230.9762740990714, Episodes: 7469, Steps: 2579858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGVhdcGGoyQF"
      },
      "source": [
        "# Новый раздел"
      ],
      "id": "bGVhdcGGoyQF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gniReYQvoy5y"
      },
      "source": [
        "# Новый раздел"
      ],
      "id": "gniReYQvoy5y"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upper-problem"
      },
      "source": [
        ""
      ],
      "id": "upper-problem",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tracked-latter"
      },
      "source": [
        ""
      ],
      "id": "tracked-latter",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reliable-japan"
      },
      "source": [
        ""
      ],
      "id": "reliable-japan",
      "execution_count": null,
      "outputs": []
    }
  ]
}